# GEMM performance analysis

 survey the dependence of GEMM performance (General Matrix-Matrix multiply) operations on parameters such as (a) Matrix Size, (b) Block size (for algorithms relying on "blocking" practices), and (c) Thread count in an OpenMP implementation.

The recommended starting point for this exploration is the blocked GEMM implementation in subdirectory DenseAlgebra/GEMM_Test_0_10 in our code repository, which should compile/run with the most ease across all platforms, but you are welcome to use some of the later, assembly-optimized blocked implementations if you can successfully build them, e.g. DenseAlgebra/GEMM_Test_1_2_avx2 or DenseAlgebra/GEMM_Test_1_2_avx2.

Specifically, you are asked to benchmark the following combinations of parameters:

Test 3 different matrix resolutions. For example, you could use matrix sizes 1024x1024, 2048x2048, and 4096x4096 (assuming the computer you are running the tests on is fast enough to complete the test for the highest resolution at a reasonable time). You are free to venture into smaller or larger matrix sizes, if you prefer, among the 3 resolutions you choose. Note that smaller matrices might make cache effects be more evident (matrices might start fitting in local caches), while larger resolutions might make parallelism easier (more work to do, easier to distribute on threads). 
The implementation you were pointed to as a starting point uses a "blocking" concept to accelerate the matrix-matrix multiply operation. The block size (which has been 32x32 or 64x64 in most of our examples) is also a parameter you can modify. Experiment with 3 different block sizes (it is recommended that you try powers of 2, i.e. 16x16, 32x32, 64x64, 128x128 ...) to examine how this choice affects parallel performance.
Examine how the number of active threads used (which can be controlled via the environmental variable OMP_NUM_THREADS) influences the scaling performance across different matrix sizes and different block sizes. At the very minimum, experiment with using just 1 thread, and with using all available threads on your computer (which is OpenMP's default behavior). A third option of using some intermediate number of threads would be desirable, but not mandatory. 
Create a table with the parallel performance in your experiments. This table may include, for example, 18 different timings (3 resolutions, 3 block sizes, one/all threads) or 27 timings if you choose to use another intermediate core count in your experiment.  Contrast these timings with the MKL-optimized version (DenseAlgebra/GEMM_Test_0_1) for the corresponding matrix resolutions and number of threads (note: the MKL version does not allow customization of "block sizes"; just use it as-is). Provide some commentary on notable observations resulting from the numbers you collect, any any thoughts on possible explanations. For example:

- Does operating at certain (large or small) matrix sizes seem to make it less important what the size of blocks is (for performance)?
- Are you seeing the same changes in parameters such as block size helping performance in some resolutions and hindering performance in others?
- For some of the resolutions/block sizes in your experiment, are you witnessing speed-up that is not quite proportional with the number of cores used when using all vs. one thread? 

Notes on including the source-code with your deliverable:  If you don't do the optional part below, and you didn't make any nontrivial changes (i.e. just changing sizes of matrices and blocks), you don't need to include your source code, although it's certainly welcome to do so, especially if you have added or modified Makefiles. You must include source code if you tackle the optional part, below.

Optional (Bonus) Task: This assignment will include an optional/bonus component, that you can choose to complete, if you so desire. It is optional in the sense that you can absolutely still achieve a perfect grade (a "5") without doing this task, if everything else (described previously) is carried out in a satisfactory way. Although the maximum grade for this assignment will still remain a "5", this optional component can serve as a "bonus" of sorts, in the following sense: if you do it correctly, this part can help forgive any deductions (up to 1.5point) that you might have had on other parts of this assignment: for example, if you turned in the assignment late, or if some other part of your assignment was deficient. Again, let us emphasize that you can absolutely still get a perfect grade without including this optional bit. 

The Optional/Bonus task is as follows: Implement and analyze a variant of the GEMM algorithm where the matrices being multiplied are not square matrices, but are rectangular matrices instead (the product matrix could still be square, if you make the two factors being multiplied rectangular)! Two examples:

Multiply a 1024x2048 matrix with a 2048x4092 matrix, producing a 1024x4092 matrix as a result, or
Multiply a 1024x2048 matrix with a 2048x1024 matrix, producing a 1024x1024 matrix as a result.
You should use matrices with dimensions that are power of two, and different (in terms of number of rows/columns) by no more than a factor of 4 (all the examples above fall into that category). You don't need to evaluate lots of different sizes; just a specific size for A, a specific size for B, and a specific size for the product C=A*B should suffice. In doing this, you'll need to change the code of our benchmark to accommodate this, in more than a few ways: You'll need to adjust how you allocate and cast matrices to account for the different row/column sizes, you'll need to adjust the reference implementation (MKL?) to account for the rectangular dimension of the factors, and similarly make sure that the number of blocks across rows/columns might need to differ (among other necessary changes).  It is recommended that if you use blocking, you keep the "blocks" themselves as square matrices (e.g. 32x32, or 64x64). Once you have achieved a correct implementation, compare its performance with one of the earlier benchmarks that used square matrices as factors, and comment on any performance discrepancies. It might help if you choose two benchmarks for which the number of multiplications is the same: for reference, when you multiply a MxN matrix with an NxP matrix, you need to perform MxNxP multiplications. 